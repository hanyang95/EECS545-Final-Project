{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian LSTM for Anomaly Detection in TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "We upload the dataset and extract the Receiving Tank and Heating Tank temperature variables. We compute the moving average of the points to reduce the dimension without losing the overall trend of the data. As our time series present a pattern, we chose to work on 2,000 points. We also sphere the data using sklearn to speed up the training phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_csv('train_1500000_seed_11_vars_23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT = dataset[['Time','RT_temperature.T']].copy()\n",
    "RT.set_index('Time',inplace=True)\n",
    "RT = RT.values\n",
    "\n",
    "HT = dataset[['Time','HT_temperature.T']].copy()\n",
    "HT.set_index('Time',inplace=True)\n",
    "HT = HT.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT = np.mean(RT.reshape(-1,39),1)\n",
    "RT = RT[0:2000].reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "RT_scaled = scaler.fit_transform(RT)\n",
    "\n",
    "HT = np.mean(HT.reshape(-1,39),1)\n",
    "HT = HT[0:2000].reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "HT_scaled = scaler.fit_transform(HT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose which variable you want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = HT_scaled\n",
    "\n",
    "n,d = scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning and Parameters Definition\n",
    "\n",
    "We choose to split the data into 80% training. \n",
    "The function window allows us to reshape the data in function of the window size we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(data,window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    i=0\n",
    "    while (i + window_size) <= len(data) - 1:\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "        \n",
    "        i+=1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "window_size = 20\n",
    "hidden_layer = 50\n",
    "learning_rate = 0.01\n",
    "n_params = 4*(2*hidden_layer + hidden_layer**2) + hidden_layer + 1\n",
    "\n",
    "\n",
    "elbo_loss = []\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1600, 20, 1)\n",
      "y_train shape: (1600, 1)\n",
      "X_test shape: (380, 20, 1)\n",
      "y_test shape: (380, 1)\n"
     ]
    }
   ],
   "source": [
    "ntrain = n - int(0.2*scaled.shape[0])\n",
    "X, y = window(scaled, window_size)\n",
    "\n",
    "X_train = np.array(X[:ntrain])\n",
    "y_train = np.array(y[:ntrain])\n",
    "t_train = np.arange(X_train.shape[0])\n",
    "\n",
    "X_test = np.array(X[ntrain:])\n",
    "y_test = np.array(y[ntrain:])\n",
    "t_test = np.arange(X_test.shape[0])\n",
    "\n",
    "\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"y_train shape: \" + str(y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "particle_tensor = tf.Variable(tf.truncated_normal([1,n_params], stddev=0.05)) \n",
    "\n",
    "finish = 0\n",
    "# Weights for the input gate\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "Wi_gate = tf.reshape(particle_tensor[:, start:finish], (1, 1, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer ** 2\n",
    "Wi_hidden = tf.reshape(particle_tensor[:, start: finish], (1, hidden_layer, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "bi = tf.reshape(particle_tensor[:, start: finish], (1, 1, hidden_layer))\n",
    "\n",
    "#Weights for the forget gate\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "Wf_gate = tf.reshape(particle_tensor[:, start:finish], (1, 1, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer ** 2\n",
    "Wf_hidden = tf.reshape(particle_tensor[:, start: finish], (1, hidden_layer, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "bf = tf.reshape(particle_tensor[:, start: finish], (1, 1, hidden_layer))\n",
    "bf_biased = tf.reshape(tf.Variable(3*tf.ones([hidden_layer])), (1, 1, hidden_layer))\n",
    "\n",
    "#Weights for the output gate\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "Wo_gate = tf.reshape(particle_tensor[:, start:finish], (1, 1, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer ** 2\n",
    "Wo_hidden = tf.reshape(particle_tensor[:, start: finish], (1, hidden_layer, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "bo = tf.reshape(particle_tensor[:, start: finish], (1, 1, hidden_layer))\n",
    "\n",
    "#weights for the memory cell\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "Wc_gate = tf.reshape(particle_tensor[:, start:finish], (1, 1, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer ** 2\n",
    "Wc_hidden = tf.reshape(particle_tensor[:, start: finish], (1, hidden_layer, hidden_layer))\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "bc = tf.reshape(particle_tensor[:, start: finish], (1, 1, hidden_layer))\n",
    "\n",
    "#Output layer weights\n",
    "start = finish\n",
    "finish = start + hidden_layer\n",
    "weights_output = tf.reshape(particle_tensor[:, start:finish], (1, hidden_layer, 1))\n",
    "start = finish\n",
    "finish = start + 1\n",
    "bias_output = tf.reshape(particle_tensor[:, start: finish], (1, 1, 1))\n",
    "start = finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the different neural networks cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_cell(input,output,state, small_dim=False):\n",
    "\n",
    "    if small_dim:\n",
    "        input_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wi_gate) \n",
    "                                 + tf.einsum('ij,njk->nik', output, Wi_hidden) + bi)\n",
    "        \n",
    "        forget_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wf_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wf_hidden) + bf)\n",
    "\n",
    "        output_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wo_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wo_hidden) +bo)\n",
    "\n",
    "        memory_cell =  tf.tanh(tf.einsum('mij,njk->mnik', input, Wc_gate) \n",
    "                               + tf.einsum('ij,njk->nik', output, Wc_hidden) + bc)\n",
    "        \n",
    "        state = tf.reshape(state, [-1, 1, hidden_layer]) * forget_gate + input_gate * memory_cell\n",
    "\n",
    "        \n",
    "    else:\n",
    "        input_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wi_gate) \n",
    "                                 + tf.einsum('...nij,njk->...nik', output, Wi_hidden) + bi)\n",
    "        \n",
    "        forget_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wf_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wf_hidden) + bf)\n",
    "\n",
    "        output_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wo_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wo_hidden) +bo)\n",
    "\n",
    "        memory_cell =  tf.tanh(tf.einsum('mij,njk->mnik', input, Wc_gate) \n",
    "                               + tf.einsum('...nij,njk->...nik', output, Wc_hidden) + bc)\n",
    "        \n",
    "        state = tf.reshape(state, [-1, 1, 1, hidden_layer]) * forget_gate + input_gate * memory_cell\n",
    "\n",
    "    \n",
    "    output = output_gate * tf.tanh(state)\n",
    "    \n",
    "    return state, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_cell_peephole(input,output,state, small_dim=False):\n",
    "    if small_dim:\n",
    "        input_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wi_gate) \n",
    "                                 + tf.einsum('ij,njk->nik', output, Wi_hidden) + bi)\n",
    "        \n",
    "        forget_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wf_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wf_hidden) + bf)\n",
    "\n",
    "        output_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wo_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wo_hidden) +bo)\n",
    "\n",
    "        memory_cell =  tf.tanh(tf.einsum('mij,njk->mnik', input, Wc_gate) + bc)\n",
    "        \n",
    "        state = tf.reshape(state, [-1, 1, hidden_layer]) * forget_gate + input_gate * memory_cell\n",
    "\n",
    "        \n",
    "    else:\n",
    "        input_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wi_gate) \n",
    "                                 + tf.einsum('...nij,njk->...nik', output, Wi_hidden) + bi)\n",
    "        \n",
    "        forget_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wf_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wf_hidden) + bf)\n",
    "\n",
    "        output_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wo_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wo_hidden) +bo)\n",
    "\n",
    "        memory_cell =  tf.tanh(tf.einsum('mij,njk->mnik', input, Wc_gate) + bc)\n",
    "        \n",
    "        state = tf.reshape(state, [-1, 1, 1, hidden_layer]) * forget_gate + input_gate * memory_cell\n",
    "\n",
    "    \n",
    "    output = tf.tanh(output_gate * state)\n",
    "    \n",
    "    return state, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_cell_biased(input,output,state, small_dim=False):\n",
    "\n",
    "    if small_dim:\n",
    "        input_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wi_gate) \n",
    "                                 + tf.einsum('ij,njk->nik', output, Wi_hidden) + bi)\n",
    "        \n",
    "        forget_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wf_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wf_hidden) + bf_biased)\n",
    "\n",
    "        output_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wo_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wo_hidden) +bo)\n",
    "\n",
    "        memory_cell =  tf.tanh(tf.einsum('mij,njk->mnik', input, Wc_gate) \n",
    "                               + tf.einsum('ij,njk->nik', output, Wc_hidden) + bc)\n",
    "        \n",
    "        state = tf.reshape(state, [-1, 1, hidden_layer]) * forget_gate + input_gate * memory_cell\n",
    "\n",
    "        \n",
    "    else:\n",
    "        input_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wi_gate) \n",
    "                                 + tf.einsum('...nij,njk->...nik', output, Wi_hidden) + bi)\n",
    "        \n",
    "        forget_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wf_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wf_hidden) + bf_biased)\n",
    "\n",
    "        output_gate =  tf.sigmoid(tf.einsum('mij,njk->mnik', input, Wo_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wo_hidden) +bo)\n",
    "\n",
    "        memory_cell =  tf.tanh(tf.einsum('mij,njk->mnik', input, Wc_gate) \n",
    "                               + tf.einsum('...nij,njk->...nik', output, Wc_hidden) + bc)\n",
    "        \n",
    "        state = tf.reshape(state, [-1, 1, 1, hidden_layer]) * forget_gate + input_gate * memory_cell\n",
    "\n",
    "    \n",
    "    output = output_gate * tf.tanh(state)\n",
    "    \n",
    "    return state, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_cell(input,output, small_dim=False):\n",
    "    if small_dim:\n",
    "        output_gate =  tf.sigmoid(tf.einsum('...ij,njk->...nik', input, Wo_gate) \n",
    "                                  + tf.einsum('ij,njk->nik', output, Wo_hidden) +bo)\n",
    "    else:\n",
    "        output_gate =  tf.sigmoid(tf.einsum('...ij,njk->...nik', input, Wo_gate) \n",
    "                                  + tf.einsum('...nij,njk->...nik', output, Wo_hidden) +bo)\n",
    "\n",
    "    output = output_gate\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(window_size, architecture):\n",
    "    if architecture == RNN_cell:\n",
    "        inputs = tf.placeholder(tf.float32, [None,window_size,1])\n",
    "        targets = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        batch_state0 = tf.constant(np.zeros([1, hidden_layer], dtype = np.float32))\n",
    "        batch_output0 = tf.constant(np.zeros([1, hidden_layer], dtype = np.float32))\n",
    "\n",
    "        for k in range(window_size):\n",
    "            if k == 0:\n",
    "                batch_output = architecture(tf.reshape(inputs[:, k], (-1,1,1)), batch_state0, True)\n",
    "            else:\n",
    "                batch_output = architecture(tf.reshape(inputs[:, k], (-1,1,1)), batch_output)\n",
    "\n",
    "        outputs = tf.matmul(batch_output, weights_output) + bias_output\n",
    "        diff_tensor = tf.reshape(outputs - tf.reshape(targets, (-1, 1, 1, 1)), (-1, 1, 1))\n",
    "        mse = tf.reduce_mean(diff_tensor ** 2)\n",
    "        \n",
    "    else:\n",
    "        inputs = tf.placeholder(tf.float32, [None,window_size,1])\n",
    "        targets = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        batch_state0 = tf.constant(np.zeros([1, hidden_layer], dtype = np.float32))\n",
    "        batch_output0 = tf.constant(np.zeros([1, hidden_layer], dtype = np.float32))\n",
    "\n",
    "        #Choose which architecture to use\n",
    "        for k in range(window_size):\n",
    "            if k == 0:\n",
    "                batch_state, batch_output = architecture(tf.reshape(inputs[:, k], (-1,1,1)), batch_state0, batch_output0, True)\n",
    "            else:\n",
    "                batch_state, batch_output = architecture(tf.reshape(inputs[:, k], (-1,1,1)), batch_state, batch_output)\n",
    "\n",
    "        outputs = tf.matmul(batch_output, weights_output) + bias_output\n",
    "\n",
    "        diff_tensor = tf.reshape(outputs - tf.reshape(targets, (-1, 1, 1, 1)), (-1, 1, 1))\n",
    "        mse = tf.reduce_mean(diff_tensor ** 2)\n",
    "        \n",
    "    return targets, inputs, outputs, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose the architecture you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "architecture = RNN_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "targets, inputs, outputs, mse = output(window_size, architecture)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step = optimizer.minimize(mse)\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, epoch_counter):\n",
    "    print('Training...')\n",
    "    max_epochs = epoch_counter + epochs\n",
    "    for i in range(epochs):\n",
    "        epoch_counter += 1\n",
    "        ii = 0\n",
    "        order = np.arange(ntrain)\n",
    "        np.random.seed(i)\n",
    "        np.random.shuffle(order)\n",
    "        while (ii+batch_size) <= X_train.shape[0]:\n",
    "            epoch_loss = []\n",
    "            X_batch = X_train[order][ii:ii+batch_size]\n",
    "            y_batch = y_train[order][ii:ii+batch_size]\n",
    "\n",
    "\n",
    "            session.run(step, feed_dict = {inputs:X_batch, targets: y_batch})\n",
    "            o, c = session.run([outputs, mse], feed_dict = {inputs:X_batch, targets: y_batch})\n",
    "\n",
    "            epoch_loss.append(c)\n",
    "\n",
    "            ii+= batch_size\n",
    "\n",
    "        elbo_loss.append(np.mean(epoch_loss))\n",
    "        print('Epoch {}/{}'.format(epoch_counter, max_epochs), ' Current loss: {}'.format(elbo_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/20  Current loss: 0.7669117450714111\n",
      "Epoch 2/20  Current loss: 0.5388635993003845\n",
      "Epoch 3/20  Current loss: 0.10538101941347122\n",
      "Epoch 4/20  Current loss: 0.08163900673389435\n",
      "Epoch 5/20  Current loss: 0.07637578248977661\n",
      "Epoch 6/20  Current loss: 0.033758051693439484\n",
      "Epoch 7/20  Current loss: 0.06191442906856537\n",
      "Epoch 8/20  Current loss: 0.04613608121871948\n",
      "Epoch 9/20  Current loss: 0.029628220945596695\n",
      "Epoch 10/20  Current loss: 0.030433237552642822\n",
      "Epoch 11/20  Current loss: 0.02373000979423523\n",
      "Epoch 12/20  Current loss: 0.04943377152085304\n",
      "Epoch 13/20  Current loss: 0.06760505586862564\n",
      "Epoch 14/20  Current loss: 0.03330322355031967\n",
      "Epoch 15/20  Current loss: 0.0176177266985178\n",
      "Epoch 16/20  Current loss: 0.032783277332782745\n",
      "Epoch 17/20  Current loss: 0.044800229370594025\n",
      "Epoch 18/20  Current loss: 0.03397064283490181\n",
      "Epoch 19/20  Current loss: 0.04631730914115906\n",
      "Epoch 20/20  Current loss: 0.02042626217007637\n"
     ]
    }
   ],
   "source": [
    "if architecture == RNN_cell:\n",
    "    epochs = 20\n",
    "else:\n",
    "    epochs = 100\n",
    "train(epochs, len(elbo_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = session.run(outputs, feed_dict = {inputs:X_test, targets: y_test}).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse the sphering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_scaled = scaler.inverse_transform(y_predict).flatten()\n",
    "y_test_scaled = scaler.inverse_transform(y_test).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.square(y_predict_scaled[:128] - y_test_scaled[:128]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is: 4.054698290394015\n"
     ]
    }
   ],
   "source": [
    "print('The MSE is: ' + str(MSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
